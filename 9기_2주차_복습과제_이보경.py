# -*- coding: utf-8 -*-
"""9기_2주차_복습과제_이보경.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ms1_sEFTafIV1y_gcQJq6E-UNLrxfJUN
"""

import pandas as pd
import numpy as np
import seaborn as sns # 시각화를 위한 라이브러리
import matplotlib.pyplot as plt
import calendar
from datetime import datetime

"""#Part 1. 파이썬 기본
사용 데이터셋 :  https://www.kaggle.com/c/bike-sharing-demand/data

"""

#colab에 실습(train.csv) 파일 업로드
from google.colab import files
import io
train_data = files.upload()
train=pd.read_csv(io.BytesIO(train_data["train.csv"]))

"""### [1-1] 
datetime 열을 년-월-일, 시간 이렇게 분리해서 새로운 tempDate 라는 이름의 열을 만들어주세요.
* ex) 2011-01-01 00:00:00  ->  2011-01-01 , 00:00:00	
"""

#datetime 열을, 년-월-일, 시간 분리
train['tempDate']= train['datetime'].str.split().astype('string')
train['tempDate']

"""### [1-2] 
1-1에서 분리한 tempDate열을 가지고, train 데이터에 year, month, day, weekday 열을 추가해주세요.
"""

#분리한 tempDate를 가지고 year, month, day, weekday column 추가
train['year']= train.tempDate.apply(lambda x : x[2:6])
train['month']= train.tempDate.apply(lambda x : x[7:9])
train['day']= train.tempDate.apply(lambda x : x[10:12])
train['weekday']= pd.to_datetime(train['tempDate'].apply(lambda x : x[2:12])).dt.day_name()
train.head()

"""### [1-3] 
1-2에서 만든 year, month, day, hour의 문자형 데이터를 숫자형 데이터로 변환해주세요.
"""

#분리한 형태의 문자형 데이터를 숫자형 데이터로 변환
train['year']= train['year'].astype(int)
train['month']= train['month'].astype(int)
train['day']= train['day'].astype(int)

"""### [1-4] 
이제, 필요없는 tempDate 열을 삭제해 주시고, train.csv을 위에서부터 5행 출력해주세요.
"""

#필요없는 tempDate열 삭제 후, train을 위에서부터 5행 출력
train.drop(['tempDate'], axis = 1, inplace = True)
train.head()

"""### [1-5] 
월별로 count를 내림차순 정렬해 주세요.
"""

train.columns

#월별로 count 내림차순 정렬
train.groupby(by=['month'], as_index = False).sum(['count']).sort_values(by = 'count', ascending = False)

"""### [1-6] 
1-5를 통해, 6월에 count가 가장 많은 것을 확인할 수 있었습니다! 6월 데이터만 추출하여, June 변수에 저장해주세요.
"""

train=train[['datetime', 'count', 'month','season', 'holiday', 'workingday', 'weather', 'temp',
       'atemp', 'humidity', 'windspeed', 'casual', 'registered',
       'year', 'day', 'weekday']]

#6월 데이터만 추출하여 June 변수에 저장
June = train[train['month'] == 6]
June

"""### [1-7]
6월달 중 가장 count가 많은 날짜(day)를 내림차순으로 정렬해주세요.
"""

# 6월달 중 가장 count가 많은 날짜 파악
# June.sort_values(by = 'count', ascending = False)
June.groupby(by=['day'], as_index = False).sum(['count']).sort_values(by = 'count', ascending = False)

"""# Part 2 데이터 전처리

한국환경공단에서 발표한 미세먼지 관련 자료를 분석하려고 데이터를 사용하려고 합니다. <p>
'SeoulHourlyAvgAirPollution.csv' 을 불러와 pollution 으로 저장해주세요.
"""

#colab에 SeoulHourlyAvgAirPollution.csv 파일 업로드
from google.colab import files
import io
SeoulHourlyAvgAirPollution_data=files.upload()
pollution =pd.read_csv(io.BytesIO(SeoulHourlyAvgAirPollution_data["SeoulHourlyAvgAirPollution.csv"]))

"""### [2-1] Label Encoding
'측정소명' 변수를 레이블 인코딩하고, 레이블 인코딩한 값을 기존 데이터 프레임에 '측정소명_인코딩'라는 열로 추가해보세요.
"""

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
encoder.fit(pollution['측정소명'])
pollution['측정소명_인코딩'] = encoder.transform(pollution['측정소명'])

"""### [2-2] 결측치, 중복값 처리
한국환경공단에서 발표한 미세먼지 관련 자료를 분석하려고 데이터를 사용하려고 합니다.우선, 칼럼별 결측치의 개수를 구해주세요.
"""

# 칼럼별 결측치 개수 확인
pollution.isnull().sum()

"""#### a. 결측치 처리하기
결측치를 대체하거나, 제외하려고 합니다. 아래의 조건에 맞게 데이터를 수정해주세요. (inplace=True)

1) '이산화질소농도'는 결측치를 제거 <p>
2) '이산화질소농도'는 데이터에서 제외 <p>
3) '일산화탄소농도'는 바로 위 행의 데이터로 대체 <p>
4) '아황산가스' 는 결측치 값을 0으로 대체 (조건: replace() 메소드 사용) <p>
5) '미세먼지'의 결측치는 열의 평균값으로 대체 <p>
6) '초미세먼지'의 결측치는 최빈값으로 대체
"""

from scipy.stats import mode 

pollution['이산화질소농도(ppm)'].dropna(inplace=True)
pollution.drop(['오존농도(ppm)'], axis = 1, inplace = True)
pollution['일산화탄소농도(ppm)'].fillna(method = 'pad', inplace=True)
pollution['아황산가스(ppm)'].replace(to_replace = np.nan, value = 0, inplace=True)
pollution['미세먼지(㎍/㎥)'] = pollution['미세먼지(㎍/㎥)'].fillna(pollution['미세먼지(㎍/㎥)'].mean(), inplace=True)
pollution['초미세먼지(㎍/㎥)'] = pollution['초미세먼지(㎍/㎥)'].fillna(mode(pollution['초미세먼지(㎍/㎥)'])[0][0], inplace=True)

"""#### b. 중복값 처리하기
DUPLICATED.csv 를 불러와, duplicates 에 저장해주세요.

duplicates를 분석해보니, 같은 사람에 대해서 중복된 데이터가 존재합니다. <p>
'NAME' 과 'ID' 가 모두 중복이면, 처음 나온 데이터가 올바른 값입니다. <p>
'NAME' 은 중복이지만, 'ID'가 중복이지 않으면, 잘못된 데이터입니다. <p>
잘못된 데이터는 삭제해주시고, 중복값을 적절하게 처리하여 데이터를 수정해주세요.
"""

DUPLICATED_data=files.upload()
duplicates = pd.read_csv(io.BytesIO(DUPLICATED_data["DUPLICATED.csv"]))

duplicates.drop_duplicates(subset = ['NAME', 'ID'], keep = 'first')
duplicates.drop_duplicates(['NAME'], keep = 'first')

"""###[2-3] 불균형 데이터 처리
## <서술형 문항>
Pollution 데이터을 분류 하려고 하는데, 불균형 데이터 처리 단계를 거쳐야 합니다.
불균형 데이터가 무엇인지 설명하고, Undersampling 과 Oversampling 이 무엇인지 간단하게 설명해주세요. (간단히만 적어주시면 됩니다!)

> 불균형 데이터란 정상 범주의 관측치 수와 이상 범주의 관측치 수가 현저히 차이나는 데이터를 말합니다. 보통 이상 데이터를 분류하는 경우가 많기 때문에 불균형 데이터 세트는 이상 데이터를 정확히 찾아내지 못할 수 있다는 점이 문제가 됩니다.  
UnderSampling은 다수 범주의 데이터를 소수 범주의 데이터 수에 맞게 줄이는 샘플링 방식입니다. 계산 시간이 감소하고, 유의미한 데이터만 남는다는 장점이 있지만, 정보가 손실될 수 있다는 단점이 있습니다. 샘플링 종류로는 random undersampling과 tomek's link, CNN이 있습니다.  
Oversampling은 소수 범주의 데이터를 다수 범주의 데이터 수에 맞게 늘리는 샘플링 방식입니다. 정보 손실이 없고 대부분 언더 샘플링에 비해 높은 분류 정확도를 보인다는 장점이 있지만, 계산 시간이 증가하고 과적합 가능성이 존재하며 이상치에 민감하다는 단점이 있습니다. 종류로는 random oversampling, ADASYN, SMOTE 등이 있습니다.

#### under sampling
'train' 데이터프레임을 사용하여, x는 'count', 'registered', y는 'workingday'로 x 와 y 값을 설정하세요. 위 x,y 에 대하여 under sampliing 기법 중 하나를 선택하여 불균형 데이터 처리를 진행하고 어떻게 데이터가 변화하였는데 살펴보세요. 
(hint: shape, value_counts 등을 통해 샘플링 전후 데이터 개수의 변화를 알 수 있습니다.)<p>
"""

from imblearn.under_sampling import *

x = train[['count', 'registered']]
y = train[['workingday']]
print((y==1).sum())
print((y==0).sum())
print(x.shape)
print(y.shape)

x_resampled, y_resampled = RandomUnderSampler(random_state =0).fit_resample(x,y)
print(x_resampled.shape)
print(y_resampled.shape)
print((y_resampled==1).sum())
print((y_resampled==0).sum())

"""> random undersampling을 진행하였고, shape과 sum 함수를 적용해보니 샘플링 이후 y가 1인 데이터와 y가 0인 데이터의 개수가 같아진 것을 알 수 있습니다.

###[2-4] Feature Scaling
날씨와 ('weather' 칼럼) 과 전체 대여된 자전거 수 ('count' 칼럼) 을 비교하기 위하여 Feature Scaling 하고자 합니다. <p>
두 변수 모두 정규화를 적용한 후, 'weather scaled' 및 'count scaled' 으로 저장하여, 각 변수 정규화 저용 전 후 평균과 분산을 비교해주세요.
"""

from sklearn.preprocessing import StandardScaler
scaler2 = StandardScaler()

train_weather = train['weather'].values.reshape(-1,1)
scaler2.fit(train_weather)
train['weather scaled'] = scaler2.transform(train_weather)

from sklearn.preprocessing import MinMaxScaler
scaler1 = MinMaxScaler()

train_count = train['count'].values.reshape(-1,1)
scaler1.fit(train_count)
train['count scaled'] = scaler1.transform(train_count)

print('Weather 의 정규화 적용 전 평균과 분산 비교')
print(train[['weather', 'weather scaled']].mean())
print(train[['weather', 'weather scaled']].var())

print('Count 의 정규화 적용 전 평균과 분산 비교')
print(train[['count', 'count scaled']].mean())
print(train[['count', 'count scaled']].var())

"""> 새로 만들어진 두 변수의 평균은 0에 가까워졌고, 분산은 1에 가까워졌습니다. """